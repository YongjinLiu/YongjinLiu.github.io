---
title: "Emotion_Dictionary_Learning_with_Modality_Attentions_for_Mixed_Emotion_Exploration"
collection: publications
permalink: /publication/2023_Emotion_Dictionary_Learning_with_Modality_Attentions_for_Mixed_Emotion_Exploration
excerpt: ''
date: 2023-11-20
venue: 'IEEE Transactions on Affective Computing'
tags:
  - Journal Publications
citation: 'Fang Liu, Pei Yang, Yezhi Shu, Fei Yan, Guanhua Zhang, Yong-Jin Liu*, Emotion Dictionary Learning with Modality Attentions for Mixed Emotion Exploration in IEEE Transactions on Affective Computing, doi: 10.1109/TAFFC.2023.3334520.'
---

Abstract: Multi-modal emotion analysis, as an important direction in affective computing, has attracted increasing attention in recent years. Most existing multi-modal emotion recognition studies are targeted at a classification task that aims to assign a specific emotion category to a combination of several heterogeneous input data, including multimedia signals and physiological signals. Compared to single-class emotion recognition, a growing number of recent psychological evidence suggests that different discrete emotions may co-exist at the same time, which promotes the development of mixed-emotion recognition to identify a mixture of basic emotions.Although most current studies treat it as a multi-label classification task, in this work, we focus on a challenging situation where both positive and negative emotions are presented simultaneously, and propose a multi-modal mixed emotion recognition framework, namely EmotionDict. The key characteristics of our EmotionDict include the following. (1) Inspired by the psychological evidence that such a mixed state can be represented by combinations of basic emotions, we address mixed emotion recognition as a label distribution learning task. An emotion dictionary has been designed to disentangle the mixed emotion representations into a weighted sum of a set of basic emotion elements in a shared latent space and their corresponding weights. (2) While many existing emotion distribution studies are built on a single type of multimedia signal (such as text, image, audio, and video), we incorporate physiological and overt behavioral multi-modal signals, including electroencephalogram (EEG), peripheral physiological signals, and facial videos, which directly display the subjective emotions. These modalities have diverse characteristics given that they are related to the central or peripheral nervous system, and the motor cortex. (3) We further design auxiliary tasks to learn modality attentions for modality integration. Experiments on two datasets show that our method outperforms existing state-of-the-art approaches on mixed-emotion recognition.


[Download paper here](http://yongjinliu.github.io/files/2023_Emotion_Dictionary_Learning_with_Modality_Attentions_for_Mixed_Emotion_Exploration.pdf)

[More information]
Recommended citation: Fang Liu, Pei Yang, Yezhi Shu, Fei Yan, Guanhua Zhang, Yong-Jin Liu*, Emotion Dictionary Learning with Modality Attentions for Mixed Emotion Exploration in IEEE Transactions on Affective Computing, doi: 10.1109/TAFFC.2023.3334520.




